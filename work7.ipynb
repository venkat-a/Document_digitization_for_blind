{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac96c591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.15.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (10.2.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.5.3)\n",
      "Collecting scikeras\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "INFO: pip is looking at multiple versions of scikeras to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading scikeras-0.12.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow numpy matplotlib scikit-learn pillow pandas scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559fac7b",
   "metadata": {},
   "source": [
    "This code snippet imports necessary libraries and modules for building and evaluating deep learning and machine learning models using TensorFlow, Keras, and Scikit-learn:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "### Description:\n",
    "\n",
    "1. **TensorFlow and Keras Imports:**\n",
    "   - `import tensorflow as tf`: Imports the TensorFlow library.\n",
    "   - `from tensorflow.keras.models import Sequential, Model`: Imports the `Sequential` and `Model` classes from `tensorflow.keras.models`, used for creating neural network models.\n",
    "   - `from tensorflow.keras.layers import ...`: Imports various layer types (`Conv2D`, `MaxPooling2D`, `Flatten`, `Dense`, `Dropout`, `BatchNormalization`, `GlobalAveragePooling2D`) from `tensorflow.keras.layers`. These are fundamental building blocks for constructing neural networks.\n",
    "\n",
    "2. **Regularization and Wrapper Imports:**\n",
    "   - `from tensorflow.keras.regularizers import l2`: Imports the L2 regularization method from `tensorflow.keras.regularizers`, useful for preventing overfitting in neural networks.\n",
    "   - `from scikeras.wrappers import KerasClassifier`: Imports the `KerasClassifier` wrapper from `scikeras.wrappers`. This allows using Keras models as classifiers within the scikit-learn framework, enabling easy integration of Keras models with scikit-learn's utilities like `GridSearchCV`.\n",
    "\n",
    "3. **Pre-trained Models Imports:**\n",
    "   - `from tensorflow.keras.applications import ResNet50, EfficientNetB0`: Imports pre-trained models (`ResNet50` and `EfficientNetB0`) from `tensorflow.keras.applications`. These models are powerful, pre-trained convolutional neural networks (CNNs) often used as feature extractors or as the base models for transfer learning tasks.\n",
    "\n",
    "4. **Scikit-learn Imports:**\n",
    "   - `from sklearn.linear_model import LogisticRegression`: Imports the `LogisticRegression` class from `sklearn.linear_model`, which is a traditional machine learning model used for classification tasks.\n",
    "   - `from sklearn.metrics import accuracy_score`: Imports the `accuracy_score` function from `sklearn.metrics`, used to evaluate the accuracy of classification models.\n",
    "\n",
    "5. **Utility Libraries:**\n",
    "   - `import numpy as np`: Imports the NumPy library, providing support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "\n",
    "### Purpose:\n",
    "This set of imports is typically used in machine learning and deep learning projects where:\n",
    "- **TensorFlow and Keras** are used for building and training neural networks.\n",
    "- **Scikit-learn** is used for various machine learning tasks such as model evaluation (`GridSearchCV`) and traditional machine learning models (`LogisticRegression`).\n",
    "- **NumPy** is used for efficient numerical computation with large datasets, often used for data preprocessing and manipulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87f8b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 18:14:26.929501: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-20 18:14:30.348574: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-20 18:14:30.348629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-20 18:14:30.615515: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-20 18:14:31.180045: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 18:14:47.174680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5dc2d",
   "metadata": {},
   "source": [
    "ImageDataGenerator:\n",
    "\n",
    "Purpose:\n",
    "\n",
    "ImageDataGenerator is a utility in TensorFlow's Keras API that generates batches of augmented/normalized data from image data. It's used for data preprocessing and augmentation during training of deep learning models, particularly convolutional neural networks (CNNs).\n",
    "Functionality:\n",
    "\n",
    "Data Augmentation: It provides a way to augment images on-the-fly during the training process. Augmentation techniques can include random rotation, resizing, shearing, flipping, etc., which helps in improving the model's robustness and generalization.\n",
    "Normalization: It can normalize pixel values of images (e.g., scaling pixel values to [0, 1] or [-1, 1]) which can help in improving convergence during training.\n",
    "Batch Generation: It generates batches of image data (along with their corresponding labels) that can be fed into the model during training or evaluation.\n",
    "Efficiency: By generating augmented data dynamically, it helps in efficiently utilizing memory and computing resources, especially when dealing with large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6089e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0a742",
   "metadata": {},
   "source": [
    "Certainly! Here's a concise description of what each part of the code does without rewriting it:\n",
    "\n",
    " Code Description:\n",
    "\n",
    "Data Augmentation for Training (`train_datagen`):\n",
    "   - `rotation_range=40`: Randomly rotate images within the range of 0 to 40 degrees.\n",
    "   - `width_shift_range=0.2` and `height_shift_range=0.2`: Randomly shift images horizontally and vertically by up to 20% of the image width or height.\n",
    "   - `shear_range=0.2`: Apply shear-based transformations with a shear intensity of up to 20%.\n",
    "   - `zoom_range=0.2`: Randomly zoom into images by up to 20%.\n",
    "   - `horizontal_flip=True`: Randomly flip images horizontally.\n",
    "   - `fill_mode='nearest'`: Strategy to fill in newly created pixels, which is 'nearest' in this case.\n",
    "   - `rescale=1./255`: Normalize pixel values to the range [0, 1].\n",
    "\n",
    "Data Generator for Training (`train_generator`):\n",
    "   - `flow_from_directory`: Creates a data generator from images stored in a directory.\n",
    "   - `'images/training'`: Path to the directory containing training images.\n",
    "   - `target_size=(224, 224)`: Resizes all images to 224x224 pixels.\n",
    "   - `batch_size=32`: Number of images in each batch of data.\n",
    "   - `class_mode='categorical'`: Mode for categorical classification, which expects labels in a categorical format.\n",
    " Data Generator for Validation (`test_generator`):\n",
    "   - `flow_from_directory`: Creates a data generator from images stored in a directory.\n",
    "   - `'images/testing'`: Path to the directory containing validation images.\n",
    "   - `target_size=(224, 224)`: Resizes all images to 224x224 pixels, matching the training data.\n",
    "   - `batch_size=32`: Number of images in each batch of data.\n",
    "   - `class_mode='categorical'`: Mode for categorical classification, which expects labels in a categorical format.\n",
    "   - `rescale=1./255`: Normalize pixel values to the range [0, 1], ensuring consistency with the training data preprocessing.\n",
    "\n",
    "Purpose:\n",
    "Training Data Augmentation:** Enhances the training dataset by applying various transformations to increase the diversity of images seen by the model during training, thereby improving its ability to generalize.\n",
    "  \n",
    "Validation Data Preprocessing:** Ensures that the validation dataset is only normalized (`rescaled`), without introducing any additional variations or augmentations, which ensures that model evaluation is based on the original, unmodified images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eddd3d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2393 images belonging to 3 classes.\n",
      "Found 597 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the data augmentation for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation data should not be augmented, only rescaled\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create the data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'images/training',  # Path to your training data\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'images/testing',  # Path to your validation data\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79cc5c7",
   "metadata": {},
   "source": [
    "Input Parameters:\n",
    "\n",
    "input_shape: Tuple defining the shape of input images (default: (224, 224, 3) for images of size 224x224 pixels with 3 channels).\n",
    "num_classes: Number of classes for classification (default: 3).\n",
    "learning_rate: Learning rate for the optimizer (default: 0.001).\n",
    "l2_reg: L2 regularization strength (default: 0.001).\n",
    "activation: Activation function used in layers (default: 'relu').\n",
    "optimizer: Optimizer used during model compilation (default: 'adam').\n",
    "Model Architecture:\n",
    "\n",
    "Sequential Model: Linear stack of layers.\n",
    "Conv2D Layers: Convolutional layers with specified number of filters, kernel size (3, 3), and activation function (activation). Each convolutional layer is followed by batch normalization (BatchNormalization()) to stabilize and accelerate training.\n",
    "MaxPooling2D Layers: Pooling layers to reduce spatial dimensions.\n",
    "Flatten Layer: Flattens the input, necessary before feeding into fully connected layers.\n",
    "Dense Layers: Fully connected layers with specified number of units (256, 128) and activation function (activation), applying L2 regularization (kernel_regularizer=l2(l2_reg)) to the kernel weights.\n",
    "Dropout Layers: Regularization technique to prevent overfitting (Dropout(0.5)).\n",
    "Output Layer: Dense layer with num_classes units and softmax activation for multi-class classification.\n",
    "Optimizer Selection:\n",
    "\n",
    "Based on the optimizer parameter, selects one of the following optimizers with the specified learning_rate: Adam ('adam'), SGD ('sgd'), RMSprop ('rmsprop'), or Adagrad ('adagrad').\n",
    "Compilation:\n",
    "\n",
    "Compiles the model with the selected optimizer, categorical crossentropy loss function ('categorical_crossentropy'), and metrics (['accuracy']).\n",
    "Return:\n",
    "\n",
    "Returns the compiled Keras model ready for training.\n",
    "Purpose:\n",
    "Model Architecture: Defines a baseline CNN model suitable for image classification tasks, incorporating convolutional, pooling, batch normalization, dropout, and fully connected layers.\n",
    "\n",
    "Flexibility: Allows customization of input shape, number of classes, learning rate, regularization strength, activation functions, and optimizer choice, providing flexibility to experiment with different configurations.\n",
    "\n",
    "Training: Once instantiated and compiled, this model can be trained on image data using methods like fit with data generated by ImageDataGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56fb7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model(input_shape=(224, 224, 3), num_classes=3, learning_rate=0.001, l2_reg=0.001, activation='relu', optimizer='adam'):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation=activation, input_shape=input_shape, kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation=activation, kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation=activation, kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(256, (3, 3), activation=activation, kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(256, activation=activation, kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation=activation, kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        opt = tf.keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba4f0f",
   "metadata": {},
   "source": [
    "Import KerasClassifier:\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier: Imports the KerasClassifier class from the scikeras.wrappers module. This wrapper allows integrating Keras models into scikit-learn pipelines and utilizing scikit-learn's functionalities like GridSearchCV.\n",
    "Create KerasClassifier instance:\n",
    "\n",
    "model = KerasClassifier(model=create_baseline_model, input_shape=(224, 224, 3), num_classes=3): Creates an instance of KerasClassifier.\n",
    "Parameters:\n",
    "model=create_baseline_model: Specifies the Keras model function (create_baseline_model in this case) that constructs the neural network model.\n",
    "input_shape=(224, 224, 3): Defines the input shape expected by the model.\n",
    "num_classes=3: Specifies the number of classes for the classification task.\n",
    "Purpose:\n",
    "\n",
    "This allows leveraging scikit-learn's utilities such as GridSearchCV for hyperparameter tuning and cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f53f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "model = KerasClassifier(model=create_baseline_model, input_shape=(224, 224, 3), num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1225f",
   "metadata": {},
   "source": [
    "dictionary provided is structured for use with GridSearchCV in scikit-learn to tune hyperparameters of a Keras model wrapped with KerasClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a72d8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'model__learning_rate': [0.0001, 0.01],\n",
    "    'model__l2_reg': [0.0001, 0.01],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'model__optimizer': ['adam', 'sgd'],\n",
    "    'fit__epochs': [1],\n",
    "    'fit__batch_size': [32]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24c672",
   "metadata": {},
   "source": [
    "perform hyperparameter tuning using GridSearchCV with a KerasClassifier model wrapped around a Keras model function (create_baseline_model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f9aab61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=adam; total time=  52.8s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=adam; total time=  48.2s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=adam; total time=  47.7s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=sgd; total time=  48.0s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=sgd; total time=  48.7s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=sgd; total time=  48.0s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=adam; total time=  47.8s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=adam; total time=  48.3s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=adam; total time=  49.0s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=sgd; total time=  48.4s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=sgd; total time=  47.9s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=sgd; total time=  48.4s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=adam; total time=  48.3s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=adam; total time=  48.1s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=adam; total time=  48.3s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=sgd; total time=  48.1s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=sgd; total time=  47.7s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=sgd; total time=  48.5s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=adam; total time=  49.1s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=adam; total time=  49.0s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=adam; total time=  48.4s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=sgd; total time=  48.7s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=sgd; total time=  48.4s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=relu, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=sgd; total time=  47.8s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=adam; total time=  48.6s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=adam; total time=  49.2s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=adam; total time=  49.1s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=sgd; total time=  47.8s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=sgd; total time=  47.1s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.0001, model__optimizer=sgd; total time=  46.8s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=adam; total time=  47.0s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=adam; total time=  48.1s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=adam; total time=  48.2s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=sgd; total time=  47.6s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=sgd; total time=  48.7s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.0001, model__learning_rate=0.01, model__optimizer=sgd; total time=  47.8s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=adam; total time=  47.5s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=adam; total time=  47.8s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=adam; total time=  47.9s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=sgd; total time=  48.4s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=sgd; total time=  48.5s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.0001, model__optimizer=sgd; total time=  47.7s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=adam; total time=  47.3s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=adam; total time=  48.1s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=adam; total time=  48.2s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=sgd; total time=  47.5s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=sgd; total time=  47.2s\n",
      "[CV] END fit__batch_size=32, fit__epochs=1, model__activation=tanh, model__l2_reg=0.01, model__learning_rate=0.01, model__optimizer=sgd; total time=  48.2s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 48 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n48 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 893, in _fit_and_score\n    estimator.fit(X_train, **fit_params)\nTypeError: KerasClassifier.fit() missing 1 required positional argument: 'y'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m grid_search_result \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/model_selection/_search.py:947\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    945\u001b[0m     )\n\u001b[0;32m--> 947\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:536\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    530\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m     )\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 48 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n48 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 893, in _fit_and_score\n    estimator.fit(X_train, **fit_params)\nTypeError: KerasClassifier.fit() missing 1 required positional argument: 'y'\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=2, n_jobs=1)\n",
    "grid_search_result = grid_search.fit(train_generator, validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4714a",
   "metadata": {},
   "source": [
    " best Hyper paramertes selected after tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcda869",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best: {grid_search_result.best_score_} using {grid_search_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "storing best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a935261",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c1d510",
   "metadata": {},
   "source": [
    "Model configuration using the best hyperparameters found through GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecfc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = create_baseline_model(\n",
    "    input_shape=(224, 224, 3),\n",
    "    num_classes=3,\n",
    "    learning_rate=best_params['model__learning_rate'],\n",
    "    l2_reg=best_params['model__l2_reg'],\n",
    "    activation=best_params['model__activation'],\n",
    "    optimizer=best_params['model__optimizer']\n",
    ")\n",
    "\n",
    "history_final = final_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=test_generator,\n",
    "    epochs=best_params['fit__epochs'],\n",
    "    batch_size=best_params['fit__batch_size'],\n",
    "    callbacks=[reduce_lr, early_stopping, lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea45f9",
   "metadata": {},
   "source": [
    "Input Parameters:\n",
    "\n",
    "input_shape: Tuple defining the shape of input images (default: (224, 224, 3) for images of size 224x224 pixels with 3 channels).\n",
    "num_classes: Number of classes for classification (default: 3).\n",
    "learning_rate: Learning rate for the optimizer (default: 0.001).\n",
    "activation: Activation function used in Dense layers (default: 'relu').\n",
    "optimizer: Optimizer used during model compilation (default: 'adam').\n",
    "Base ResNet50 Model Initialization:\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape): Loads the ResNet50 model pre-trained on ImageNet without the top (fully connected) layers. The input_shape parameter specifies the shape of input images expected by ResNet50.\n",
    "Model Customization:\n",
    "\n",
    "GlobalAveragePooling2D Layer: Adds a global average pooling layer to reduce the spatial dimensions of the input from ResNet50.\n",
    "Dense Layer: Adds a fully connected Dense layer with 128 units and specified activation function (activation).\n",
    "Dropout Layer: Applies dropout regularization to prevent overfitting by randomly setting a fraction of input units to 0 during training (Dropout(0.5)).\n",
    "Output Layer: Adds a Dense layer with num_classes units and softmax activation function for multi-class classification.\n",
    "Freezing Base Layers:\n",
    "\n",
    "for layer in base_model.layers: and layer.trainable = False: Freezes the weights of all layers in the base ResNet50 model, preventing them from being updated during training. This technique is useful when using pre-trained models to avoid destroying their learned representations.\n",
    "Optimizer Selection and Compilation:\n",
    "\n",
    "Optimizer Choice: Based on the optimizer parameter, selects one of the following optimizers with the specified learning_rate: Adam ('adam'), SGD ('sgd'), RMSprop ('rmsprop'), or Adagrad ('adagrad').\n",
    "Model Compilation: Compiles the model with the selected optimizer, categorical crossentropy loss function ('categorical_crossentropy'), and metrics (['accuracy']).\n",
    "Return:\n",
    "\n",
    "Returns the compiled Keras model (model), which is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402375c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet50_model(input_shape=(224, 224, 3), num_classes=3, learning_rate=0.001, activation='relu', optimizer='adam'):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation=activation)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        opt = tf.keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7783502",
   "metadata": {},
   "source": [
    "\n",
    "model = KerasClassifier(model=create_baseline_model, input_shape=(224, 224, 3), num_classes=3): Creates an instance of KerasClassifier.\n",
    "Parameters:\n",
    "model=create_baseline_model: Specifies the Keras model function (create_baseline_model in this case) that constructs the neural network model.\n",
    "input_shape=(224, 224, 3): Defines the input shape expected by the model.\n",
    "num_classes=3: Specifies the number of classes for the classification task.\n",
    "Purpose:\n",
    "\n",
    "This allows leveraging scikit-learn's utilities such as GridSearchCV for hyperparameter tuning and cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(model=create_resnet50_model, input_shape=(224, 224, 3), num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a7ef4",
   "metadata": {},
   "source": [
    "dictionary provided is structured for use with GridSearchCV in scikit-learn to tune hyperparameters of a Keras model wrapped with KerasClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f8c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'model__learning_rate': [0.0001, 0.01],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'model__optimizer': ['adam', 'sgd'],\n",
    "    'fit__epochs': [1],\n",
    "    'fit__batch_size': [32]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d77155",
   "metadata": {},
   "source": [
    "perform hyperparameter tuning using GridSearchCV with a KerasClassifier model wrapped around a Keras model function (create_baseline_model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=2, n_jobs=1)\n",
    "grid_search_result = grid_search.fit(train_generator, validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b0ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    " best Hyper paramertes selected after tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff8fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best: {grid_search_result.best_score_} using {grid_search_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408899d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "storing best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b4ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea253d51",
   "metadata": {},
   "source": [
    "create_resnet50_model Function Call:\n",
    "Calls the create_resnet50_model function to instantiate a ResNet50-based model.\n",
    "Parameters:\n",
    "input_shape=(224, 224, 3): Specifies the shape of input images.\n",
    "num_classes=3: Defines the number of classes for classification.\n",
    "learning_rate=best_params['model__learning_rate']: Uses the best learning rate determined by GridSearchCV.\n",
    "activation=best_params['model__activation']: Uses the best activation function determined by GridSearchCV.\n",
    "optimizer=best_params['model__optimizer']: Uses the best optimizer determined by GridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = create_resnet50_model(\n",
    "    input_shape=(224, 224, 3),\n",
    "    num_classes=3,\n",
    "    learning_rate=best_params['model__learning_rate'],\n",
    "    activation=best_params['model__activation'],\n",
    "    optimizer=best_params['model__optimizer']\n",
    ")\n",
    "\n",
    "history_final = final_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=test_generator,\n",
    "    epochs=best_params['fit__epochs'],\n",
    "    batch_size=best_params['fit__batch_size'],\n",
    "    callbacks=[reduce_lr, early_stopping, lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd33ba",
   "metadata": {},
   "source": [
    "Base Model Initialization:\n",
    "\n",
    "Loads the EfficientNetB0 model pre-trained on ImageNet without its top layers (include_top=False). This model serves as a feature extractor.\n",
    "Customization:\n",
    "\n",
    "GlobalAveragePooling2D Layer: Reduces the spatial dimensions of the extracted features.\n",
    "Dense Layer with Dropout: Adds a dense layer followed by dropout for regularization, helping prevent overfitting.\n",
    "Output Layer: Final dense layer with softmax activation, suitable for multi-class classification tasks.\n",
    "Freezing Base Layers:\n",
    "\n",
    "Freezes the weights of the pre-trained EfficientNetB0 layers to retain the learned features and prevent them from being updated during training.\n",
    "Optimizer Selection and Compilation:\n",
    "\n",
    "Depending on the chosen optimizer ('adam', 'sgd', 'rmsprop', 'adagrad'), configures the model with the specified learning rate for optimization. Compiles the model with categorical crossentropy loss and accuracy metric.\n",
    "Purpose:\n",
    "Efficiency and Performance: EfficientNetB0 is known for its balance between computational efficiency and accuracy, making it suitable for various image classification tasks, especially when computational resources are limited.\n",
    "\n",
    "Transfer Learning: Leveraging pre-trained weights from ImageNet allows the model to quickly adapt to new datasets with potentially fewer labeled examples, improving generalization and reducing the need for extensive training data.\n",
    "\n",
    "Flexibility: Parameters such as input shape, number of classes, learning rate, activation function, and optimizer can be customized to adapt the model to specific requirements and optimize its performance for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe84fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_efficientnet_model(input_shape=(224, 224, 3), num_classes=3, learning_rate=0.001, activation='relu', optimizer='adam'):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation=activation)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        opt = tf.keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc62087",
   "metadata": {},
   "source": [
    "model = KerasClassifier(model=create_baseline_model, input_shape=(224, 224, 3), num_classes=3): Creates an instance of KerasClassifier.\n",
    "Parameters:\n",
    "model=create_baseline_model: Specifies the Keras model function (create_baseline_model in this case) that constructs the neural network model.\n",
    "input_shape=(224, 224, 3): Defines the input shape expected by the model.\n",
    "num_classes=3: Specifies the number of classes for the classification task.\n",
    "Purpose:\n",
    "\n",
    "This allows leveraging scikit-learn's utilities such as GridSearchCV for hyperparameter tuning and cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(model=create_efficientnet_model, input_shape=(224, 224, 3), num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62727ec",
   "metadata": {},
   "source": [
    "dictionary provided is structured for use with GridSearchCV in scikit-learn to tune hyperparameters of a Keras model wrapped with KerasClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72732b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'model__learning_rate': [0.0001, 0.01],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'model__optimizer': ['adam', 'sgd'],\n",
    "    'fit__epochs': [1],\n",
    "    'fit__batch_size': [32]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29676ef0",
   "metadata": {},
   "source": [
    "perform hyperparameter tuning using GridSearchCV with a KerasClassifier model wrapped around a Keras model function (create_baseline_model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=2, n_jobs=1)\n",
    "grid_search_result = grid_search.fit(train_generator, validation_data=test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b289eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    " best Hyper paramertes selected after tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74132e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best: {grid_search_result.best_score_} using {grid_search_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "storing best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe554bd8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Model Architecture Setup:\n",
    "   - The `create_efficientnet_model` function initializes an EfficientNetB0 model pretrained on ImageNet (`weights='imagenet'`) but excludes the top classification layers (`include_top=False`). This allows the model to serve as a feature extractor.\n",
    "\n",
    "Customization for Task:\n",
    "   - After the base EfficientNetB0 model, a GlobalAveragePooling2D layer is added to reduce the spatial dimensions of the features extracted by the base model.\n",
    "   - Next, a Dense layer with 128 units and an activation function (typically 'relu' or 'tanh') specified by the `activation` parameter is applied. This layer helps in learning task-specific representations.\n",
    "   - Dropout regularization with a rate of 0.5 is applied to the Dense layer to prevent overfitting during training.\n",
    "   - Finally, a Dense layer with `num_classes` units and a softmax activation function is added for the final classification output, where `num_classes` is specified by the user.\n",
    "\n",
    "Freezing Base Layers:\n",
    "   - All layers from the base EfficientNetB0 model are set to be non-trainable (`layer.trainable = False`). This approach ensures that the pre-trained weights of the base model remain intact during training, allowing the model to benefit from the learned features of ImageNet without modifying them extensively.\n",
    "\n",
    "Optimizer Selection and Compilation:\n",
    "   - The optimizer is chosen based on the `optimizer` parameter (`'adam'`, `'sgd'`, `'rmsprop'`, or `'adagrad'`) with the learning rate specified by `learning_rate`. This step configures the model for training using the selected optimizer.\n",
    "   - The loss function is set to `'categorical_crossentropy'` since it is commonly used for multi-class classification tasks.\n",
    "   - Metrics for evaluation during training are defined as `'accuracy'` to monitor the model's performance.\n",
    "\n",
    "Training the Final Model:\n",
    "Training Process:\n",
    "   - The `final_model.fit()` function initiates the training process for the finalized EfficientNetB0 model.\n",
    "   - It takes as input:\n",
    "     - `train_generator`: A generator providing batches of training data.\n",
    "     - `validation_data=test_generator`: A generator for validation data, typically separate from the training data.\n",
    "     - `epochs=best_params['fit__epochs']`: Number of epochs to train the model, as determined by hyperparameter tuning.\n",
    "     - `batch_size=best_params['fit__batch_size']`: Number of samples per gradient update, also optimized through hyperparameter tuning.\n",
    "     - `callbacks=[reduce_lr, early_stopping, lr_scheduler]`: Optional callbacks to monitor and adjust the training process dynamically.\n",
    "\n",
    "Purpose:\n",
    "Efficient Feature Extraction: Utilizes EfficientNetB0's efficient architecture for extracting meaningful features from images, leveraging pre-trained weights from ImageNet to enhance model performance.\n",
    "  \n",
    "Customization:Allows flexibility in configuring the model architecture and optimizer settings based on the best parameters determined through hyperparameter tuning.\n",
    "  \n",
    "Training Efficiency: Ensures that the model trains effectively with reduced risk of overfitting, thanks to dropout regularization and fine-tuned optimizer settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = create_efficientnet_model(\n",
    "    input_shape=(224, 224, 3),\n",
    "    num_classes=3,\n",
    "    learning_rate=best_params['model__learning_rate'],\n",
    "    activation=best_params['model__activation'],\n",
    "    optimizer=best_params['model__optimizer']\n",
    ")\n",
    "\n",
    "history_final = final_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=test_generator,\n",
    "    epochs=best_params['fit__epochs'],\n",
    "    batch_size=best_params['fit__batch_size'],\n",
    "    callbacks=[reduce_lr, early_stopping, lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3f08b",
   "metadata": {},
   "source": [
    "train three different models (baseline_model, resnet50_model, and efficientnet_model) using the same training and validation data generators (train_generator and test_generator) over 30 epochs, while also applying the same set of callbacks (reduce_lr, early_stopping, and lr_scheduler) to each training session.\n",
    "\n",
    "Training Each Model:\n",
    "\n",
    "Each model (baseline_model, resnet50_model, efficientnet_model) is trained using the fit() method.\n",
    "train_generator: Provides batches of training data during the training process.\n",
    "validation_data=test_generator: Uses batches of validation data for evaluating the model's performance after each epoch.\n",
    "epochs=30: Specifies the number of training epochs for each model.\n",
    "callbacks=[reduce_lr, early_stopping, lr_scheduler]: Incorporates callbacks to adjust the learning rate (reduce_lr), implement early stopping (early_stopping), and potentially customize the learning rate schedule (lr_scheduler) during training.\n",
    "Purpose of Callbacks:\n",
    "\n",
    "Reduce LR on Plateau (reduce_lr): Adjusts the learning rate when the model's performance plateaus, potentially improving convergence.\n",
    "Early Stopping (early_stopping): Halts training early if the validation loss stops improving, preventing overfitting and saving computational resources.\n",
    "Custom Learning Rate Scheduler (lr_scheduler): Optionally adjusts the learning rate schedule based on predefined conditions or epochs.\n",
    "\n",
    "Training Efficiency:\n",
    "By using generators (train_generator and test_generator), the training process efficiently handles large datasets without loading them entirely into memory, which is crucial for tasks involving large-scale image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6559059",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_baseline = baseline_model.fit(\n",
    "    train_generator, \n",
    "    validation_data=test_generator, \n",
    "    epochs=30, \n",
    "    callbacks=[reduce_lr, early_stopping, lr_scheduler]\n",
    ")\n",
    "history_resnet50 = resnet50_model.fit(\n",
    "    train_generator, \n",
    "    validation_data=test_generator, \n",
    "    epochs=30, \n",
    "    callbacks=[reduce_lr, early_stopping, lr_scheduler]\n",
    ")\n",
    "history_efficientnet = efficientnet_model.fit(\n",
    "    train_generator, \n",
    "    validation_data=test_generator, \n",
    "    epochs=30, \n",
    "    callbacks=[reduce_lr, early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3352c",
   "metadata": {},
   "source": [
    "The ensemble_predict function provided is designed to perform ensemble prediction using a set of pre-trained models and a test data generator\n",
    "\n",
    "Ensemble Learning: Combines predictions from multiple models to improve overall prediction accuracy and robustness.\n",
    "\n",
    "Logistic Regression Meta Model: Uses logistic regression to learn how to best combine the predictions from individual models, leveraging their strengths and compensating for weaknesses.\n",
    "\n",
    "Scalability: Efficiently handles predictions using generators (test_generator), suitable for large datasets where loading all data into memory at once is impractical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, test_generator):\n",
    "    X = np.concatenate([model.predict(test_generator) for model in models], axis=1)\n",
    "    y = test_generator.classes\n",
    "    \n",
    "    meta_model = LogisticRegression(max_iter=1000)\n",
    "    meta_model.fit(X, y)\n",
    "    \n",
    "    X_test = np.concatenate([model.predict(test_generator) for model in models], axis=1)\n",
    "    y_pred = meta_model.predict(X_test)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d259a",
   "metadata": {},
   "source": [
    "Certainly! Here’s a concise description of how the ensemble prediction works without writing any code:\n",
    "Ensemble Prediction Overview:\n",
    "\n",
    "Ensemble prediction combines the predictions of multiple machine learning models to improve accuracy and robustness. Here’s a high-level overview of the process:\n",
    "\n",
    "Models Selection:\n",
    "   - Choose a set of pre-trained models (`final_baseline_model`, `resnet50_model`, `efficientnet_model`) that have been trained on similar or different datasets.\n",
    "\n",
    "Test Data Preparation:\n",
    "   - Use a generator (`test_generator`) to provide batches of test data. This approach is efficient for handling large datasets without loading them entirely into memory.\n",
    "\n",
    "Ensemble Prediction Function:\n",
    "   - Define a function (`ensemble_predict`) that:\n",
    "     - Takes a list of models and a test data generator as input.\n",
    "     - Uses each model to generate predictions (`model.predict(test_generator)`).\n",
    "     - Concatenates these predictions into a matrix (`X`) where each column represents predictions from a different model.\n",
    "     - Retrieves the true class labels (`y`) from the test data generator.\n",
    "\n",
    "Meta Model (Optional):\n",
    "   - Optionally, a meta-model (e.g., logistic regression) can be trained on the concatenated predictions (`X`) and true labels (`y`) to learn how to best combine the predictions from individual models.\n",
    "\n",
    "Final Prediction:\n",
    "   - Use the trained meta-model (if applicable) to predict the final class labels for the test data (`y_pred`). Alternatively, directly use the combined predictions for final predictions.\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Enhanced Performance: Ensemble methods leverage diverse models to improve prediction accuracy and generalization, compared to individual models.\n",
    "  \n",
    "Flexibility:Ensemble methods can combine models trained on different architectures or with different hyperparameters, providing a robust solution to complex tasks.\n",
    "\n",
    "Scalability:By using generators and efficient prediction methods, ensemble prediction can handle large datasets and complex models efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96771ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [final_baseline_model, resnet50_model, efficientnet_model]\n",
    "ensemble_predictions = ensemble_predict(models, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Averaged Ensembled F1 score of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f303620",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels = test_generator.classes\n",
    "ensemble_accuracy = accuracy_score(correct_labels, ensemble_predictions)\n",
    "print(f'Ensemble Accuracy: {ensemble_accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
