{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396a88d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.15.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (10.2.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.5.3)\n",
      "Collecting scikeras\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "INFO: pip is looking at multiple versions of scikeras to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading scikeras-0.12.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow numpy matplotlib scikit-learn pillow pandas scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339df58",
   "metadata": {},
   "source": [
    "imports various libraries and modules. It includes TensorFlow and Keras for deep learning model creation, preprocessing tools, layer definitions, callbacks, and optimization. Scikeras is used for integrating Keras models with Scikit-learn. Additionally, it imports logging for tracking the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63b8bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 01:14:14.403781: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-22 01:14:47.173861: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-22 01:14:47.173928: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-22 01:14:51.569321: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-22 01:14:59.321058: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-22 01:15:19.136119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.optimizers import Adam ,RMSprop\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9e0c7",
   "metadata": {},
   "source": [
    "random seed for NumPy and TensorFlow to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccdc47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dbbf0d",
   "metadata": {},
   "source": [
    "ets up directories for training, testing, and validation data. It creates image data generators for training and testing datasets with specified augmentations for the training data to improve generalization. The flow_from_directory method is used to load images from directories into the generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14743a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2393 images belonging to 3 classes.\n",
      "Found 597 images belonging to 3 classes.\n",
      "Found 597 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'images/training'\n",
    "test_dir = 'images/testing'\n",
    "val_dir = 'images/testing'\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308b901",
   "metadata": {},
   "source": [
    "create_baseline_model to build a baseline CNN model. It specifies input shape, number of classes, and learning rate. It includes convolutional, pooling, flatten, and dense layers with optional batch normalization and dropout for regularization. The model is compiled with an optimizer, loss function, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344b7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model(input_shape=(224, 224, 3), num_classes=3, learning_rate=0.001, l2_reg=0.001, activation='relu', optimizer='adam'):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation=activation, input_shape=input_shape, kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation=activation),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation=activation),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=activation, kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer_instance = tf.keras.optimizers.get(optimizer)\n",
    "    optimizer_instance.learning_rate = learning_rate\n",
    "\n",
    "    model.compile(optimizer=optimizer_instance, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08254097",
   "metadata": {},
   "source": [
    "Intializing call backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7744cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initializes base line  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec965422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 01:19:22.933738: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-22 01:19:39.185716: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-22 01:19:39.185995: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-22 01:19:39.187470: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-22 01:19:39.187669: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-22 01:19:39.187818: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-22 01:19:40.274379: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-22 01:19:40.274648: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-22 01:19:40.274819: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-22 01:19:40.274930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "baseline_model = create_baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22ce27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897eaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 01:21:00.620103: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-06-22 01:21:39.312510: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fee815fad10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-22 01:21:39.312567: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-06-22 01:21:39.519356: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1719019299.670677   13870 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 398s 5s/step - loss: 3.3975 - accuracy: 0.6389 - val_loss: 2.8756 - val_accuracy: 0.5812 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "75/75 [==============================] - 55s 734ms/step - loss: 2.5764 - accuracy: 0.7681 - val_loss: 2.7037 - val_accuracy: 0.6734 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "75/75 [==============================] - 54s 724ms/step - loss: 2.0823 - accuracy: 0.7948 - val_loss: 4.0674 - val_accuracy: 0.4858 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "75/75 [==============================] - 55s 729ms/step - loss: 1.7310 - accuracy: 0.8061 - val_loss: 1.9993 - val_accuracy: 0.6717 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "75/75 [==============================] - 54s 717ms/step - loss: 1.4854 - accuracy: 0.8253 - val_loss: 3.2697 - val_accuracy: 0.4858 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history_baseline = baseline_model.fit(train_generator, validation_data=test_generator, epochs=5, callbacks=[reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d7cc4a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Importing CosineDecay:\n",
    "Imports the `CosineDecay` class from TensorFlow Keras to implement a learning rate schedule that decays following a cosine curve, optimizing training efficiency.\n",
    "\n",
    "Setting Parameters:\n",
    "Defines the batch size, number of epochs, input image dimensions, and number of output classes, which are crucial for configuring the training process.\n",
    "\n",
    "Data Augmentation:\n",
    "Configures data augmentation for the training images to enhance model generalization and normalizes pixel values for both training and validation images to ensure consistent input data.\n",
    "\n",
    "Creating Simplified ResNet50 Model:\n",
    "Defines a simplified ResNet50 model by adding custom layers on top of a pre-trained base, freezing the base layers, and setting up a cosine decay schedule for the learning rate to optimize training.\n",
    "\n",
    "Training the Model:\n",
    "Creates and compiles the ResNet50 model, sets up early stopping to prevent overfitting, and trains the model using the training and validation data, with progress monitored by validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6739c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 1s 0us/step\n",
      "Epoch 1/5\n",
      "74/74 [==============================] - 63s 744ms/step - loss: 1.3440 - accuracy: 0.4795 - val_loss: 0.7769 - val_accuracy: 0.5069\n",
      "Epoch 2/5\n",
      "74/74 [==============================] - 54s 723ms/step - loss: 0.7651 - accuracy: 0.5125 - val_loss: 0.7434 - val_accuracy: 0.4861\n",
      "Epoch 3/5\n",
      "74/74 [==============================] - 54s 725ms/step - loss: 0.7337 - accuracy: 0.5091 - val_loss: 0.7274 - val_accuracy: 0.4878\n",
      "Epoch 4/5\n",
      "74/74 [==============================] - 53s 712ms/step - loss: 0.7247 - accuracy: 0.4998 - val_loss: 0.7138 - val_accuracy: 0.5122\n",
      "Epoch 5/5\n",
      "74/74 [==============================] - 54s 724ms/step - loss: 0.7163 - accuracy: 0.5066 - val_loss: 0.7088 - val_accuracy: 0.5191\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 3\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Simplified ResNet50 model\n",
    "def create_simplified_resnet50_model(input_shape=(224, 224, 3), num_classes=3):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    initial_learning_rate = 0.001\n",
    "    lr_schedule = CosineDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=1000,\n",
    "        alpha=0.1\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train the ResNet50 model\n",
    "resnet50_model = create_simplified_resnet50_model()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "resnet50_history = resnet50_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.n // batch_size,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95306c3",
   "metadata": {},
   "source": [
    "Data Augmentation:\n",
    "Configures data augmentation for the training images to enhance model generalization by applying various transformations like rescaling, shearing, zooming, horizontal flipping, rotation, and shifting. The validation images are only rescaled.\n",
    "\n",
    "Simplified EfficientNetB0 Model Creation:\n",
    "Defines a simplified EfficientNetB0 model by adding custom layers on top of the pre-trained EfficientNetB0 base, including global average pooling, dense, and dropout layers. The base model's layers are frozen to retain pre-trained weights.\n",
    "\n",
    "Learning Rate Schedule:\n",
    "Implements a cosine decay schedule for the learning rate, starting at an initial value and gradually reducing it following a cosine curve to optimize the training process.\n",
    "\n",
    "Training the EfficientNetB0 Model:\n",
    "Creates and compiles the EfficientNetB0 model, sets up early stopping to monitor validation loss and prevent overfitting, and trains the model using the training and validation data generators, with progress monitored by validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2809a50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 01:38:19.018748: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_2/block2b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 61s 742ms/step - loss: 1.7788 - accuracy: 0.4972 - val_loss: 1.0501 - val_accuracy: 0.5208\n",
      "Epoch 2/5\n",
      "74/74 [==============================] - 56s 750ms/step - loss: 0.9071 - accuracy: 0.5066 - val_loss: 0.8024 - val_accuracy: 0.5139\n",
      "Epoch 3/5\n",
      "74/74 [==============================] - 53s 719ms/step - loss: 0.7757 - accuracy: 0.4994 - val_loss: 0.7406 - val_accuracy: 0.4878\n",
      "Epoch 4/5\n",
      "74/74 [==============================] - 53s 713ms/step - loss: 0.7306 - accuracy: 0.4989 - val_loss: 0.7175 - val_accuracy: 0.5087\n",
      "Epoch 5/5\n",
      "74/74 [==============================] - 53s 715ms/step - loss: 0.7135 - accuracy: 0.5133 - val_loss: 0.7064 - val_accuracy: 0.5104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Simplified EfficientNetB0 model\n",
    "def create_simplified_efficientnet_model(input_shape=(224, 224, 3), num_classes=3):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    initial_learning_rate = 0.001\n",
    "    lr_schedule = CosineDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=1000,\n",
    "        alpha=0.1\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train the EfficientNetB0 model\n",
    "efficientnet_model = create_simplified_efficientnet_model()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "efficientnet_history = efficientnet_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.n // batch_size,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4f9bf",
   "metadata": {},
   "source": [
    "\n",
    "Create the ResNet50 Model:\n",
    "Defines a ResNet50-based model by initializing it with pre-trained weights and excluding the top layers. The last 30 layers of the base model are unfrozen for fine-tuning.\n",
    "\n",
    "Adding Custom Layers:\n",
    "Adds a global average pooling layer, a dense layer with 256 units and ReLU activation, and a dropout layer to prevent overfitting. The final layer is a dense layer with softmax activation for classification.\n",
    "\n",
    "Compile the Model:\n",
    "Compiles the model using the Adam optimizer with a learning rate of 0.001 and categorical cross-entropy loss, setting up the model for training with accuracy as the evaluation metric.\n",
    "\n",
    "Model Instantiation:\n",
    "Creates an instance of the ResNet50 model with the defined architecture and compilation settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5335f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the ResNet50 model\n",
    "def create_resnet50_model(input_shape=(224, 224, 3), num_classes=3):\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    #Unfreeze the top layers of the base model\n",
    "    for layer in base_model.layers[-30:]:  # Unfreeze the last 30 layers\n",
    "        layer.trainable = True\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Compile the model\n",
    "    opt = Adam(learning_rate=0.001)\n",
    "    l2_reg=0.001\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_resnet50_model()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e0ab0",
   "metadata": {},
   "source": [
    "ReduceLROnPlateau Callback:\n",
    "Sets up a learning rate reduction strategy to lower the learning rate by a factor of 0.2 if the validation loss does not improve for 3 epochs, with a minimum learning rate of \\(1 \\times 10^{-6}\\).\n",
    "\n",
    "EarlyStopping Callback:\n",
    "Implements early stopping to halt training if the validation loss does not improve for 5 consecutive epochs, restoring the best weights observed during training.\n",
    "\n",
    "ModelCheckpoint Callback:\n",
    "Configures a checkpointing strategy to save the model's weights to a file named `best_model.h5` whenever the validation loss achieves a new minimum, ensuring the best model is preserved.\n",
    "\n",
    "Train the Model:\n",
    "Trains the ResNet50 model using the training data generator for 5 epochs, validating against the test data generator, and applying the specified callbacks to optimize and save the best model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39195b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.5944 - accuracy: 0.7785"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 96s 799ms/step - loss: 0.5944 - accuracy: 0.7785 - val_loss: 1.4272 - val_accuracy: 0.5142 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "75/75 [==============================] - 53s 709ms/step - loss: 0.2569 - accuracy: 0.8959 - val_loss: 2.3894 - val_accuracy: 0.4858 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "75/75 [==============================] - 63s 838ms/step - loss: 0.2104 - accuracy: 0.9219 - val_loss: 0.8714 - val_accuracy: 0.4858 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "75/75 [==============================] - 53s 697ms/step - loss: 0.2103 - accuracy: 0.9185 - val_loss: 1.4248 - val_accuracy: 0.4858 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "75/75 [==============================] - 57s 763ms/step - loss: 0.1806 - accuracy: 0.9344 - val_loss: 1.8515 - val_accuracy: 0.4858 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "history_resnet50 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[reduce_lr, early_stopping, checkpoint])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2fa05",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Create the EfficientNet Model:\n",
    "Defines an EfficientNetB0-based model by initializing it with pre-trained weights and excluding the top layers. The last 30 layers of the base model are unfrozen for fine-tuning.\n",
    "\n",
    "Adding Custom Layers:\n",
    "Adds a global average pooling layer, batch normalization, a dense layer with 256 units and ReLU activation, and a dropout layer to prevent overfitting. The final layer is a dense layer with softmax activation for classification.\n",
    "\n",
    "Compile the Model:\n",
    "Compiles the model using the RMSprop optimizer with a learning rate of 0.0001 and categorical cross-entropy loss, setting up the model for training with accuracy as the evaluation metric.\n",
    "\n",
    "Model Instantiation:\n",
    "Creates an instance of the EfficientNet model with the defined architecture and compilation settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c9c71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the EfficientNet model\n",
    "def create_efficientnet_model(input_shape=(224, 224, 3), num_classes=3):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    # Unfreeze the top layers of the base model\n",
    "    for layer in base_model.layers[-30:]:  # Unfreeze the last 30 layers\n",
    "        layer.trainable = True\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Compile the model with RMSprop optimizer\n",
    "    opt = RMSprop(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "efficientnet_model = create_efficientnet_model()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be5147",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ReduceLROnPlateau Callback:\n",
    "Configures a learning rate reduction strategy to lower the learning rate by a factor of 0.2 if the validation loss does not improve for 3 epochs, with a minimum learning rate of \\(1 \\times 10^{-5}\\) and verbose output.\n",
    "\n",
    "EarlyStopping Callback:\n",
    "Sets up early stopping to halt training if the validation loss does not improve for 5 consecutive epochs, restoring the best weights observed during training, with verbose output for progress tracking.\n",
    "\n",
    "ModelCheckpoint Callback:\n",
    "Implements a checkpointing strategy to save the model's weights to a file named `best_efficientnet_model.h5` whenever the validation loss achieves a new minimum, ensuring the best model is preserved, with verbose output.\n",
    "\n",
    "Train the Model:\n",
    "Trains the EfficientNet model using the training data generator for 5 epochs, validating against the validation data generator, and applying the specified callbacks to optimize and save the best model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "699fac15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 01:50:33.843661: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_4/block2b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - ETA: 0s - loss: 1.1131 - accuracy: 0.5537\n",
      "Epoch 1: val_loss improved from inf to 0.90728, saving model to best_efficientnet_model.h5\n",
      "75/75 [==============================] - 78s 738ms/step - loss: 1.1131 - accuracy: 0.5537 - val_loss: 0.9073 - val_accuracy: 0.5142 - lr: 1.0000e-04\n",
      "Epoch 2/5\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.5283 - accuracy: 0.8003\n",
      "Epoch 2: val_loss improved from 0.90728 to 0.86565, saving model to best_efficientnet_model.h5\n",
      "75/75 [==============================] - 54s 719ms/step - loss: 0.5283 - accuracy: 0.8003 - val_loss: 0.8656 - val_accuracy: 0.5142 - lr: 1.0000e-04\n",
      "Epoch 3/5\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.8922\n",
      "Epoch 3: val_loss did not improve from 0.86565\n",
      "75/75 [==============================] - 53s 700ms/step - loss: 0.2825 - accuracy: 0.8922 - val_loss: 0.9490 - val_accuracy: 0.5142 - lr: 1.0000e-04\n",
      "Epoch 4/5\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.1879 - accuracy: 0.9306\n",
      "Epoch 4: val_loss did not improve from 0.86565\n",
      "75/75 [==============================] - 54s 713ms/step - loss: 0.1879 - accuracy: 0.9306 - val_loss: 0.9408 - val_accuracy: 0.5142 - lr: 1.0000e-04\n",
      "Epoch 5/5\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9444\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.86565\n",
      "75/75 [==============================] - 54s 718ms/step - loss: 0.1620 - accuracy: 0.9444 - val_loss: 1.3179 - val_accuracy: 0.5142 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "checkpoint = ModelCheckpoint('best_efficientnet_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history_efficientnet = efficientnet_model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,  # Increase the number of epochs\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[reduce_lr, early_stopping, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfce21a5",
   "metadata": {},
   "source": [
    "\n",
    "Function Definition:\n",
    "Defines the function `ensemble_predict` to make predictions using an ensemble of models and a meta-model for final classification.\n",
    "\n",
    "Concatenate Predictions:\n",
    "Concatenates the predictions from each model in the ensemble for the test data along the second axis, forming a single feature set for training the meta-model.\n",
    "\n",
    "Train Meta-Model:\n",
    "Trains a logistic regression model (meta-model) using the concatenated predictions as input features and the true labels from the test generator.\n",
    "\n",
    "Predict with Meta-Model:\n",
    "Concatenates the predictions from each model again and uses the trained logistic regression meta-model to predict the final labels for the test data.\n",
    "\n",
    "Return Predictions:\n",
    "Returns the final predicted labels from the logistic regression meta-model for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1eb9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, test_generator):\n",
    "    X = np.concatenate([model.predict(test_generator) for model in models], axis=1)\n",
    "    y = test_generator.classes\n",
    "    \n",
    "    meta_model = LogisticRegression(max_iter=1000)\n",
    "    meta_model.fit(X, y)\n",
    "    \n",
    "    X_test = np.concatenate([model.predict(test_generator) for model in models], axis=1)\n",
    "    y_pred = meta_model.predict(X_test)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31e57f",
   "metadata": {},
   "source": [
    "Function Definition:\n",
    "Defines the function `get_predictions` to generate binary predictions from a given model and data generator.\n",
    "\n",
    "Model Predictions:\n",
    "Uses the model to predict probabilities for the input data provided by the data generator.\n",
    "\n",
    "Binary Conversion:\n",
    "Converts the predicted probabilities to binary labels (0 or 1) by applying a threshold of 0.5, and returns these binary predictions as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9ae6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_generator):\n",
    "    predictions = model.predict(data_generator)\n",
    "    return (predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa59c95",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Baseline Model Predictions:\n",
    "Generates binary predictions from the `baseline_model` using the `get_predictions` function and the `test_generator`.\n",
    "\n",
    "ResNet50 Model Predictions:\n",
    "Generates binary predictions from the `resnet50_model` using the `get_predictions` function and the `test_generator`.\n",
    "\n",
    "EfficientNet Model Predictions:\n",
    "Generates binary predictions from the `efficientnet_model` using the `get_predictions` function and the `test_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9a41655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 7s 341ms/step\n",
      "19/19 [==============================] - 7s 347ms/step\n",
      "19/19 [==============================] - 8s 345ms/step\n"
     ]
    }
   ],
   "source": [
    "baseline_preds = get_predictions(baseline_model, test_generator)\n",
    "resnet50_preds = get_predictions(resnet50_model, test_generator)\n",
    "efficientnet_preds = get_predictions(efficientnet_model, test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb4d5d",
   "metadata": {},
   "source": [
    "Averaging Predictions:\n",
    "Calculates the average of the binary predictions from the baseline, ResNet50, and EfficientNet models by summing their predictions and dividing by three.\n",
    "\n",
    "Rounding to Binary:\n",
    "Rounds the averaged predictions to the nearest integer (0 or 1) to obtain the final ensemble binary predictions, combining the strengths of all three models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82bd4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_preds = np.round((baseline_preds + resnet50_preds + efficientnet_preds) / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41b549",
   "metadata": {},
   "source": [
    "\n",
    "Model List:\n",
    "Creates a list of models (`baseline_model`, `resnet50_model`, and `efficientnet_model`) to be used in the ensemble.\n",
    "\n",
    "Ensemble Predictions:\n",
    "Calls the `ensemble_predict` function with the list of models and the `test_generator`, generating the final ensemble predictions using the meta-model approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6fb6668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 7s 360ms/step\n",
      "19/19 [==============================] - 7s 359ms/step\n",
      "19/19 [==============================] - 7s 354ms/step\n",
      "19/19 [==============================] - 7s 370ms/step\n",
      "19/19 [==============================] - 7s 368ms/step\n",
      "19/19 [==============================] - 7s 349ms/step\n"
     ]
    }
   ],
   "source": [
    "models = [baseline_model, resnet50_model, efficientnet_model]\n",
    "ensemble_predictions = ensemble_predict(models, test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a256e882",
   "metadata": {},
   "source": [
    "Correct Labels:\n",
    "Retrieves the true labels for the test dataset from the `test_generator`.\n",
    "\n",
    "Calculate Ensemble Accuracy:\n",
    "Uses the `accuracy_score` function from Scikit-learn to compute the accuracy of the ensemble predictions by comparing them to the correct labels.\n",
    "\n",
    "Print Accuracy:\n",
    "Prints the ensemble accuracy, formatted to two decimal places, to provide a performance measure of the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c6be5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "correct_labels = test_generator.classes\n",
    "ensemble_accuracy = accuracy_score(correct_labels, ensemble_predictions)\n",
    "print(f'Ensemble Accuracy: {ensemble_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be614588",
   "metadata": {},
   "source": [
    "\n",
    "Model Training Loop:\n",
    "Sets up a loop to train three instances of the baseline model, each with different initialization but the same architecture.\n",
    "\n",
    "Create Baseline Model:\n",
    "Creates an instance of the baseline model with specified input shape and number of classes.\n",
    "\n",
    "Early Stopping Callback:\n",
    "Sets up early stopping to monitor validation loss, halting training if it doesn't improve for 10 epochs, and restores the best model weights.\n",
    "\n",
    "Model Training:\n",
    "Trains each model using the training data generator, validating with the validation data generator, and applying the early stopping callback over a specified number of epochs and steps per epoch.\n",
    "\n",
    "Save Model Weights:\n",
    "Saves the weights of each trained model to a file, naming them uniquely based on the loop index to distinguish between different model instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "218ae820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "74/74 [==============================] - 51s 654ms/step - loss: 3.3764 - accuracy: 0.6535 - val_loss: 4.0051 - val_accuracy: 0.5191\n",
      "Epoch 2/5\n",
      "74/74 [==============================] - 49s 663ms/step - loss: 2.5584 - accuracy: 0.7747 - val_loss: 2.3609 - val_accuracy: 0.7604\n",
      "Epoch 3/5\n",
      "74/74 [==============================] - 50s 679ms/step - loss: 2.1457 - accuracy: 0.7692 - val_loss: 1.9953 - val_accuracy: 0.7309\n",
      "Epoch 4/5\n",
      "74/74 [==============================] - 50s 676ms/step - loss: 1.8074 - accuracy: 0.8107 - val_loss: 1.6514 - val_accuracy: 0.7882\n",
      "Epoch 5/5\n",
      "74/74 [==============================] - 50s 678ms/step - loss: 1.5641 - accuracy: 0.8251 - val_loss: 1.4737 - val_accuracy: 0.8194\n",
      "Epoch 1/5\n",
      "74/74 [==============================] - 53s 681ms/step - loss: 3.2040 - accuracy: 0.6739 - val_loss: 11.4639 - val_accuracy: 0.5174\n",
      "Epoch 2/5\n",
      "74/74 [==============================] - 49s 664ms/step - loss: 2.5483 - accuracy: 0.7632 - val_loss: 3.5796 - val_accuracy: 0.5208\n",
      "Epoch 3/5\n",
      "74/74 [==============================] - 51s 685ms/step - loss: 2.1669 - accuracy: 0.7857 - val_loss: 2.6795 - val_accuracy: 0.5677\n",
      "Epoch 4/5\n",
      "74/74 [==============================] - 51s 693ms/step - loss: 1.7206 - accuracy: 0.8221 - val_loss: 2.0408 - val_accuracy: 0.6840\n",
      "Epoch 5/5\n",
      "74/74 [==============================] - 48s 654ms/step - loss: 1.4743 - accuracy: 0.8230 - val_loss: 1.4489 - val_accuracy: 0.7951\n",
      "Epoch 1/5\n",
      "74/74 [==============================] - 50s 649ms/step - loss: 3.1352 - accuracy: 0.6870 - val_loss: 3.7513 - val_accuracy: 0.5295\n",
      "Epoch 2/5\n",
      "74/74 [==============================] - 51s 695ms/step - loss: 2.6048 - accuracy: 0.7802 - val_loss: 2.7651 - val_accuracy: 0.6806\n",
      "Epoch 3/5\n",
      "74/74 [==============================] - 48s 653ms/step - loss: 2.1209 - accuracy: 0.8022 - val_loss: 1.9695 - val_accuracy: 0.7396\n",
      "Epoch 4/5\n",
      "74/74 [==============================] - 50s 670ms/step - loss: 1.6776 - accuracy: 0.8153 - val_loss: 3.5004 - val_accuracy: 0.4792\n",
      "Epoch 5/5\n",
      "74/74 [==============================] - 49s 660ms/step - loss: 1.4987 - accuracy: 0.8230 - val_loss: 1.8079 - val_accuracy: 0.6337\n"
     ]
    }
   ],
   "source": [
    "num_models = 3\n",
    "for i in range(num_models):\n",
    "    model = create_baseline_model(input_shape=input_shape, num_classes=num_classes)\n",
    "    baseline_preds_{i} = get_predictions(model, test_generator)\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.n // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=val_generator.n // batch_size,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Save model weights\n",
    "    model.save_weights(f'baseline_model_weights_{i}.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2abc6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Initialize Model List:\n",
    "Creates a list to store multiple instances of the baseline model.\n",
    "\n",
    "Load Model Weights:\n",
    "Loops through to create instances of the baseline model and loads pre-trained weights from saved files for each instance, ensuring multiple versions of the model are ready for ensemble prediction.\n",
    "\n",
    "Model Predictions:\n",
    "Generates predictions from all model instances on the test data, storing the results in a list.\n",
    "\n",
    "Average Predictions:\n",
    "Calculates the ensemble predictions by averaging the probabilities from all model predictions.\n",
    "\n",
    "Determine Final Labels:\n",
    "Determines the final predicted labels by selecting the class with the highest average probability for each test sample.\n",
    "\n",
    "Calculate Ensemble Accuracy:\n",
    "Computes the accuracy of the ensemble model by comparing the final predicted labels to the true labels from the test data.\n",
    "\n",
    "Print Ensemble Accuracy:\n",
    "Outputs the calculated accuracy of the ensemble model, formatted to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c59dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59308b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15561efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 6s 312ms/step\n",
      "19/19 [==============================] - 6s 293ms/step\n",
      "19/19 [==============================] - 6s 305ms/step\n",
      "Ensemble Accuracy: 0.47\n"
     ]
    }
   ],
   "source": [
    "# Train instances of multiple models \n",
    "num_models = 3\n",
    "models = []\n",
    "for i in range(num_models):\n",
    "    model = create_baseline_model(input_shape=input_shape, num_classes=num_classes)\n",
    "    # Load pre-trained weights if available, or train here\n",
    "    model.load_weights(f'baseline_model_weights_{i}.h5')  # Ensure you have trained and saved these models\n",
    "    models.append(model)\n",
    "\n",
    "# Predict with all models\n",
    "predictions = [model.predict(test_generator, steps=np.ceil(test_generator.n / test_generator.batch_size), verbose=1) for model in models]\n",
    "\n",
    "# Ensemble predictions (average the probabilities)\n",
    "ensemble_preds = np.mean(predictions, axis=0)\n",
    "ensemble_preds_labels = np.argmax(ensemble_preds, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_labels = test_generator.classes\n",
    "ensemble_accuracy = accuracy_score(correct_labels[:len(ensemble_preds_labels)], ensemble_preds_labels)\n",
    "print(f'Ensemble Accuracy: {ensemble_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0108e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8152c25d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_preds_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ensemble_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround((\u001b[43mbaseline_preds_1\u001b[49m \u001b[38;5;241m+\u001b[39m baseline_preds_2 \u001b[38;5;241m+\u001b[39m baseline_preds_3) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline_preds_1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "ensemble_preds = np.round((baseline_preds_1 + baseline_preds_2 + baseline_preds_3) / 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455266a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels = test_generator.classes\n",
    "ensemble_accuracy = accuracy_score(correct_labels, ensemble_predictions)\n",
    "print(f'Ensemble Accuracy: {ensemble_accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
